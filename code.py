# -*- coding: utf-8 -*-
"""Submission-2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y6Co0x_6AUpVexNZqY3sVBbWJj1ySAsZ

# Import Library
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import linear_kernel
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

"""# Load Dataset"""

dataset = pd.read_csv('https://raw.githubusercontent.com/mufidatuln/Proyek-Sistem-Rekomendasi-Amazone-Product/main/dataset/amazon.csv')

"""# Univariate Eksploratory Data Analyst"""

dataset.info()

dataset.describe()

dataset.head()

"""## Cek Missing Value"""

dataset.isnull().sum()

"""Mengecek missing value dengan methode `.isnull()` untuk mengetahui berapa banyak jumlah *missing value* di setiap kolom. Dari hasil di atas dapat diketahui bahwa terdapat 2 nilai pada rating_count yang bernilai NaN."""

dataset[dataset['rating_count'].isnull()]

dataset.dropna(subset=['rating_count'], inplace=True)
print(dataset.isnull().sum())

"""Melakukan pembersihan missing value dengan methode *dropna()*"""

dataset.duplicated().sum()

"""Mengecek apakah terdapat data yang duplikat dengan method *duplicated()*. Pada data tersebut tidak ditemukan data duplikat.

## Mengubah Format Type Data
"""

dataset.info()

"""Untuk memudahkan proses tokenizer dan modeling diperlukan transformasi tipe data dari object ke tipe data float"""

dataset['discounted_price'] = dataset['discounted_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)
dataset['actual_price'] = dataset['actual_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)
dataset['discount_percentage'] = dataset['discount_percentage'].astype(str).str.replace('%','').astype(float)/100
dataset['rating_count'] = dataset['rating_count'].astype(str).str.replace(',', '').astype(float)

unique_values = dataset['rating'].unique()
print(unique_values)

"""Terdapat nilai rating yang tidak sesuai dengan format yaitu '|' sehingga diperlukan pembersihan data lebih lanjut"""

dataset['rating'] = pd.to_numeric(dataset['rating'], errors='coerce')

"""method .`to_numeric()` mengubah type data menjadi numerik dengan parameter `errors='coerce'` untuk mengembalikan nilai NaN jika format tidak sesuai. Sehingga nilai '|' pada data berubah menjadi nilai NaN."""

dataset = dataset.dropna()
print(dataset.isnull().sum())
print(dataset.info())

"""Membersihkan Kembali Data"""

dataset['sub_category'] = dataset['category'].astype(str).str.split('|').str[-1]
dataset['main_category'] = dataset['category'].astype(str).str.split('|').str[0]

"""Membuat Kolom/Fitur yaitu `sub_category` dan `main_category` untuk memisahkan dan menjadikan kategori lebih sepesifik dengan mengambil data dari fitur `category`.

# Visualisasi Data
"""

# Filter kolom-kolom numerik
numeric_cols = dataset.select_dtypes(include=['float', 'int']).columns
data = dataset[numeric_cols]
# Inisialisasi figure dan axes untuk subplot
fig, axs = plt.subplots(2, 4, figsize=(12, 6))  # Membuat grid subplot dengan 2 baris dan 4 kolom

# Flatten axes agar mudah diakses
axs = axs.flatten()

# Loop melalui setiap fitur dan plot data pada subplot yang sesuai
for i, feature in enumerate(data.columns):
    ax = axs[i]
    ax.hist(data[feature], bins=20, color='skyblue', edgecolor='black')
    ax.set_title(f'{feature} Distribution')
    ax.set_xlabel('Value')
    ax.set_ylabel('Frequency')

# Menghapus subplot yang tidak digunakan
for i in range(len(data.columns), len(axs)):
    fig.delaxes(axs[i])

# Mengatur layout subplot
plt.tight_layout()

# Menampilkan plot
plt.show()

"""Berikut adalah visualisasi persebaran data numeric dari dataset produk amazone. Dapat dilihat pada fitur `discount_price`, `actual_price` dan `rating_count` memiliki distribusi data miring ke kanan.

Sedangka untuk fitur `discount_presentage` dan `rating` memilki distribusi hampir simetris
"""

# Data
main_category_counts = dataset['main_category'].value_counts()[:11]

# Plot bar
plt.bar(range(len(main_category_counts)), main_category_counts.values)

# Tambahkan nilai di atas bar
for i, value in enumerate(main_category_counts.values):
    plt.text(i, value + 0.1, str(value), ha='center', va='bottom')

# Label dan judul
plt.ylabel('Jumlah Produk')
plt.title('Distribusi Produk Kategori Utama')

# Hilangkan label sumbu x
plt.xticks(range(len(main_category_counts)), '')

# Tampilkan plot
plt.show()

# Top 10 main categories
top_main_categories = pd.DataFrame({'Kategori Utama': main_category_counts.index, 'Jumlah Produk': main_category_counts.values})
print('Top 10 Kategori Utama')
print(top_main_categories.to_string(index=False))

"""Berdasarkan hasil tersebut dapat disimpulkan bahwa tiga kategori utama teratas adalah Elektronik, Komputer & Aksesori, dan Rumah & Dapur. Hal ini menunjukkan bahwa kategori-kategori ini populer di kalangan pelanggan. Jumlah produk dalam kategori utama lainnya cukup rendah, yang menunjukkan bahwa kategori-kategori ini tidak sepopuler tiga kategori teratas."""

# Menghitung Top Main Kategori
top = dataset.groupby(['main_category'])['rating'].mean().sort_values(ascending=False).head(10).reset_index()

# Create a bar plot
plt.bar(top['main_category'], top['rating'])

# Add labels and title
plt.xlabel('Kategori Utama')
plt.ylabel('Rating')
plt.title('Top Kategori Utama Berdasarkan Rating')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Show the plot
plt.show()
ranking = dataset.groupby('main_category')['rating'].mean().sort_values(ascending=False).reset_index()
ranking_tabel = pd.DataFrame({'Kategori Utama' : ranking['main_category'], 'Rating' : ranking['rating']})
ranking_tabel

"""Dengan melihat tabel ini, kita dapat melihat kategori utama yang diperingkat berdasarkan peringkat rata-rata. Kategori utama dengan peringkat tertinggi adalah Produk Kantor, Mainan & Permainan, dan Perbaikan Rumah, dengan peringkat di atas 4,0. Hal ini menunjukkan bahwa pelanggan pada umumnya puas dengan produk yang ditawarkan dalam kategori-kategori ini.

Intinya, sistem rekomendasi berbasis konten adalah sistem yang memanfaatkan rekomendasi pengguna yang berasal dari deskripsi barang. Sistem berbasis konten beroperasi berdasarkan premis bahwa jika pengguna telah merespons dengan baik terhadap barang dengan karakteristik tertentu, dia harus memiliki minat terhadap hal-hal yang sebanding dan dikatakan memiliki karakteristik yang sama. Cara yang paling populer untuk mengkarakterisasi fitur suatu barang adalah dengan kata kunci dan bobotnya, yang menunjukkan seberapa penting kata kunci tersebut bagi deskripsi barang.

Untuk menghitung kemiripan, digunakan rumus berikut ini.

# Data Preparosessing
"""

dataset['product_details'] = dataset['product_name']  + ' ' + dataset['about_product'] + ' ' + dataset['review_content']
dataset['product_details'] = dataset['product_details'].fillna('')
dataset['product_details']

"""Membuat kolom baru yaitu `produk_details` yang berisi beberapa kolom/fitur yaitu `product_name`, `about_product` dan `reviwe_product`"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vec = TfidfVectorizer(stop_words='english',max_df=0.95,min_df=2,ngram_range=(1,1))
tfidf_matrix = tfidf_vec.fit_transform(dataset['product_details'])
tfidf_matrix

"""Pada tahap ini dilakukan untuk mengubah teks menjadi vektor dengan fitur TF-IDF. Pada hal ini menerapkan `fit_transform` pada teks yang diberikan dalam dataset. `fit_transform` akan menghitung TF-IDF untuk setiap kata dalam teks dan menghasilkan matriks yang berisi nilai TF-IDF untuk setiap kata dalam setiap dokumen.

Hasil dari proses vektorisasi disimpan dalam variabel `tfidf_matrix`.  Hasilnya adalah matriks yang berisi representasi numerik dari teks dalam dataset, di mana setiap baris mewakili satu dokumen dan setiap kolom mewakili kata tertentu dalam kumpulan data.
"""

tfidf_matrix.shape

"""# Conten Based Recommendations

##Modeling
"""

from sklearn.metrics.pairwise import linear_kernel

cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
sim_score = 0

def content_based_recommendations(product_id, data, cosine_sim, n=5):
    # mencari indeks (baris) dari produk yang sesuai dengan product_id dalam dataset.
    idx = data[data['product_id'] == product_id].index[0]
    # mengambil similaritas kosinus antara produk yang dipilih (idx)
    # dan semua produk lain dalam dataset.
    sim_scores = list(enumerate(cosine_sim[idx]))
    # Similaritas kosinus diurutkan secara menurun dan hanya diambil n nilai teratas.
    #Langkah ini dilakukan untuk mendapatkan produk dengan similaritas tertinggi.
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:n+1]
    #Indeks produk yang terkait dengan similaritas teratas diambil dari sim_scores.
    product_indices = [i[0] for i in sim_scores]
    return data.iloc[product_indices], sim_scores

content_based_recommendations, p_sim_score = content_based_recommendations('B09LHXNZLR', dataset, cosine_sim)

"""Rekomendasi content bases ini menggunakan metode *cosine similarity*.

*Cosine similarity* adalah metrik yang digunakan untuk mengukur seberapa mirip dua vektor dalam ruang berdimensi banyak. Dalam konteks sistem rekomendasi atau pemrosesan teks, *cosine similarity* sering digunakan untuk mengukur seberapa mirip dua dokumen atau item berdasarkan representasi numerik mereka.

Dalam konteks sistem rekomendasi, *cosine similarity* sering digunakan untuk membandingkan profil pengguna dengan item-item dalam basis data untuk memberikan rekomendasi. Misalnya, dalam sistem rekomendasi berbasis konten, *cosine similarity* dapat digunakan untuk membandingkan profil preferensi pengguna dengan deskripsi atau atribut-atribut produk untuk menentukan seberapa cocok produk tersebut dengan preferensi pengguna. Semakin tinggi nilai *cosine similarity* antara profil pengguna dan produk, semakin besar kemungkinan produk tersebut direkomendasikan kepada pengguna.
"""

content_based_recommendations[['product_id','product_name']]

"""Pemanggilan `content_based_recommendations` dengan argumen 'B09LHXNZLR' pada dataset dan matriks cosine similarity (cosine_sim) menghasilkan daftar produk yang direkomendasikan berdasarkan produk dengan ID 'B09LHXNZLR'.

## Evaluasi
"""

# finding ground truth score using ratings
recommended_product_ids = content_based_recommendations['product_id'].tolist()
ratings_for_recommended_items = dataset[dataset['product_id'].isin(recommended_product_ids)]

# Define a threshold for considering an item as relevant (e.g., 4 or 5 stars)
threshold = 4

# Assign binary relevance labels based on the threshold
ground_truth = [1 if rating >= threshold else 0 for rating in ratings_for_recommended_items['rating']]

predicted_scores = [tup[1] for tup in p_sim_score]
print("Predicted Relevance Scores:", predicted_scores)

"""Program tersebut bertujuan untuk mengevaluasi seberapa baik sistem rekomendasi berbasis konten dapat memprediksi item yang relevan untuk pengguna.

1. Mencari Skor Ground Truth menggunakan Rating:

* Sistem merekomendasikan beberapa produk kepada pengguna.
* Seberapa relevan produk-produk tersebut berdasarkan rating yang diberikan oleh pengguna.

2. Menetapkan Batas untuk Mempertimbangkan Item yang Relevan:

* batas minimal rating yang akan dianggap sebagai produk yang relevan, misalnya, rating 4 atau 5 bintang.

3.  Menetapkan Label Relevansi Biner berdasarkan Ambang Batas:

* Setiap produk yang direkomendasikan diberi label 1 jika ratingnya melewati batas relevansi yang telah ditetapkan, atau 0 jika ratingnya di bawah batas tersebut.

4. Mendapatkan Skor Prediksi:

* Dari hasil rekomendasi, diperoleh skor similaritas antara produk yang direkomendasikan dan produk yang telah dipilih sebelumnya oleh pengguna.
5. Mencetak Skor Relevansi yang Diprediksi:
* Mencetak skor similaritas tersebut untuk melihat seberapa dekat produk yang direkomendasikan dengan preferensi pengguna.


Tujuan akhirnya adalah untuk melihat seberapa baik sistem rekomendasi kita dapat memprediksi preferensi pengguna dengan menganalisis skor relevansi yang diprediksi dengan skor relevansi sebenarnya berdasarkan rating yang diberikan pengguna.
"""

from sklearn.metrics import average_precision_score

ap = average_precision_score(ground_truth, predicted_scores)
print("Average Precision (AP):", ap)

"""Evaluasi yang dilakukan dalam proyek ini adalah metrik *Average Precision* (AP)

* Fungsi `average_precision_score` dijalankan dengan dua argumen:
1. `ground_truth`: Ini adalah daftar label biner yang menunjukkan relevansi sebenarnya dari setiap item yang direkomendasikan. Label ini telah ditetapkan sebelumnya berdasarkan rating yang diberikan oleh pengguna.
2. `predicted_scores`: Ini adalah skor prediksi yang menunjukkan seberapa relevan setiap item yang direkomendasikan menurut sistem. Skor ini diperoleh dari metode *cosine similarity* yang digunakan untuk merekomendasikan item.

*Average precision* memberikan gambaran tentang seberapa baik sistem rekomendasi mampu mengurutkan item yang relevan di atas yang tidak relevan. Semakin tinggi nilai a*verage precision*, semakin baik kualitas rekomendasi sistem tersebut. Nilai *average precision* berkisar antara 0 hingga 1, dengan nilai 1 menunjukkan kualitas rekomendasi yang sempurna.

Pada proyek ini *Avarage precision* memperoleh nilai 1 ynag artinya menuntukan kualitas rekomendasi yang baik.

# Collaborative Filtering Recommendations
"""

!pip install surprise

"""*Collaborative filtering* adalah metode yang digunakan dalam sistem rekomendasi untuk menghasilkan rekomendasi kepada pengguna berdasarkan penilaian (rating) atau perilaku pengguna serta kesamaan atau perbandingan dengan pengguna lain. Dalam *collaborative filtering*, rekomendasi dibuat berdasarkan pola hubungan antara pengguna dan item yang diamati dari data *riil*.

Keuntungan *collaborative filtering* adalah kemampuannya untuk merekomendasikan item yang tidak terikat oleh deskripsi atau atribut tertentu dari item, tetapi didasarkan pada pola perilaku pengguna yang sebenarnya. Namun, kerugiannya adalah ketika data penilaian pengguna sangat langka atau ketika terjadi masalah keamanan privasi, seperti masalah identifikasi pengguna, yang dapat membatasi efektivitas metode ini.

## Data Preprosessing
"""

from surprise import Dataset, Reader, accuracy
from surprise.model_selection import train_test_split
from surprise import KNNBasic

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(dataset[['user_id', 'product_id', 'rating']], reader)

# Membagi dataset menjadi set latih dan set uji
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""Dataset dipilih dari beberapa fitur yang diberikan, hanya memilih fitur `user_id`, `product_id`, dan `rating`.
Dataset kemudian dibagi menjadi set pelatihan (trainset) dan set pengujian (testset) menggunakan *train_test_split*.

## Modeling
"""

# Membangun dan melatih model collaborative filtering user-based
sim_options = {'name': 'cosine', 'user_based': True}
model = KNNBasic(sim_options=sim_options)
model.fit(trainset)

# Membuat Prediksi
predictions = model.test(testset)

"""## Evaluasi"""

# Evaluasi model
accuracy.rmse(predictions)
accuracy.mae(predictions)

predictions[0:3]

"""Model memperkirakan peringkat untuk pengguna 'AHH26HAPTOI5Z52DFLNYU5TOLWCQ' untuk item 'B08M66K48D' sekitar 4,097. Namun, peringkat yang sebenarnya ('r_ui') adalah 4,3.

Demikian pula, untuk prediksi kedua dan ketiga, model memperkirakan peringkat untuk pengguna dan item yang sesuai sekitar 4,097, tetapi peringkat sebenarnya masing-masing adalah 3,8 dan 4,0.
"""